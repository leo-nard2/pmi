{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Activation, BatchNormalization, Flatten\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_size = 3390\n",
    "event_num = 2\n",
    "droprate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DNN():\n",
    "    print(\"________DNN_________\")\n",
    "    train_input = Input(shape=(vector_size,), name='Inputlayer')\n",
    "    train_in = Dense(512, activation='relu')(train_input)\n",
    "    train_in = BatchNormalization()(train_in)\n",
    "    train_in = Dropout(droprate)(train_in)\n",
    "\n",
    "    train_in = Dense(256, activation='relu')(train_in)\n",
    "    train_in = BatchNormalization()(train_in)\n",
    "    train_in = Dropout(droprate)(train_in)\n",
    "\n",
    "    train_in = Dense(event_num)(train_in)\n",
    "    out = Activation('softmax')(train_in)\n",
    "\n",
    "    model = Model(train_input, out)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readdata():\n",
    "    path = 'pcaTrue.txt'\n",
    "    path1 = 'pcaFalse.txt'\n",
    "\n",
    "    all_matrix = []\n",
    "    all_labels = []\n",
    "\n",
    "    fp = open(path, 'r')\n",
    "    fp_false = open(path1, 'r')\n",
    "    lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.split(\" \")\n",
    "        truedata = []\n",
    "        for i in range(0, len(line)):\n",
    "            truedata.append(float(line[i]))\n",
    "        all_matrix.append(truedata)\n",
    "        all_labels.append(0)\n",
    "\n",
    "    lines2 = fp_false.readlines()\n",
    "    for line in lines2:\n",
    "        line = line.split(\" \")\n",
    "        falsedata = []\n",
    "        for i in range(0, len(line)):\n",
    "            falsedata.append(float(line[i]))\n",
    "        all_matrix.append(falsedata)\n",
    "        all_labels.append(1)\n",
    "    cc = list(zip(all_matrix, all_labels))\n",
    "    random.shuffle(cc)\n",
    "    all_matrix[:], all_labels[:] = zip(*cc)\n",
    "    return all_matrix, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(pred_type, pred_score, y_test, event_num):\n",
    "    y_one_hot = label_binarize(y_test, np.arange(event_num + 1))\n",
    "    y_one_hot = y_one_hot[:, [0, 1]]\n",
    "\n",
    "    result_auc_micro = roc_auc_score(y_one_hot, pred_score, average='micro')\n",
    "    result_auc_macro = roc_auc_score(y_one_hot, pred_score, average='macro')\n",
    "    return result_auc_micro, result_auc_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index(label_matrix, event_num, seed, CV):\n",
    "    index_all_class = np.zeros(len(label_matrix))\n",
    "    for j in range(event_num):\n",
    "        index = np.where(label_matrix == j)\n",
    "        kf = KFold(n_splits=CV, shuffle=True, random_state=seed)\n",
    "        k_num = 0\n",
    "        for train_index, test_index in kf.split(range(len(index[0]))):\n",
    "            index_all_class[index[0][test_index]] = k_num\n",
    "            k_num += 1\n",
    "    return index_all_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(feature_matrix, label_matrix, clf_type, event_num, seed, CV):\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    y_score = np.zeros((0, event_num), dtype=float)\n",
    "    label_matrix = np.array(label_matrix)\n",
    "    feature_matrix = np.array(feature_matrix)\n",
    "    index_all_class = get_index(label_matrix, event_num, seed, CV)\n",
    "\n",
    "    matrix = []\n",
    "    print(\"_____cross_validation_____\")\n",
    "\n",
    "    for k in range(CV):\n",
    "        train_index = np.where(index_all_class != k)\n",
    "        test_index = np.where(index_all_class == k)\n",
    "        pred = np.zeros((len(test_index[0]), event_num), dtype=float)\n",
    "\n",
    "        x_train = feature_matrix[train_index]\n",
    "        x_test = feature_matrix[test_index]\n",
    "        y_train = label_matrix[train_index]\n",
    "        y_test = label_matrix[test_index]\n",
    "\n",
    "        y_train_one_hot = np.array(y_train)\n",
    "        y_train_one_hot = (np.arange(y_train_one_hot.max() + 1) == y_train[:, None]).astype(dtype='float32')\n",
    "\n",
    "        y_test_one_hot = np.array(y_test)\n",
    "        y_test_one_hot = (np.arange(y_test_one_hot.max() + 1) == y_test[:, None]).astype(dtype='float32')\n",
    "\n",
    "        if clf_type == 'DNN':\n",
    "            dnn = DNN()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "            dnn.fit(x_train, y_train_one_hot, batch_size=64, epochs=100, validation_data=(x_test, y_test_one_hot),\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "            pred += dnn.predict(x_test)\n",
    "        else:\n",
    "            print(\"_______ERROR___________\")\n",
    "        pred_score = pred / 1\n",
    "        pred_type = np.argmax(pred_score, axis=1)\n",
    "        y_true = np.hstack((y_true, y_test))\n",
    "        y_pred = np.hstack((y_pred, pred_type))\n",
    "        y_score = np.row_stack((y_score, pred_score))\n",
    "\n",
    "        wfp = open(str(k) + '.txt', 'w')\n",
    "        for i in range(len(y_test)):\n",
    "            res = str(pred_score[i][0]) + ' ' + str(pred_score[i][1]) + ' ' + str(y_test[i]) + '\\n'\n",
    "            wfp.write(res)\n",
    "        wfp.close()\n",
    "\n",
    "        #########evaluate auc###########\n",
    "        result_micro, result_macro = evaluate(pred_type, pred_score, y_test, event_num)\n",
    "        print(\"idx, auc_micro, auc_macro: \", k, result_micro, result_macro)\n",
    "    result_all_micro, result_all_macro = evaluate(y_pred, y_score, y_true, event_num)\n",
    "    print(\"auc_micro_all, auc_macro_all: \", result_all_micro, result_all_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    seed = 0\n",
    "    CV = 10\n",
    "    all_matrix, all_labels = readdata()\n",
    "    cross_validation(all_matrix, all_labels, 'DNN', event_num, seed, CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____cross_validation_____\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12707 samples, validate on 1413 samples\n",
      "Epoch 1/100\n",
      "12707/12707 [==============================] - 11s 842us/sample - loss: 0.6638 - acc: 0.7113 - val_loss: 0.4659 - val_acc: 0.7955\n",
      "Epoch 2/100\n",
      "12707/12707 [==============================] - 9s 677us/sample - loss: 0.4104 - acc: 0.8207 - val_loss: 0.4337 - val_acc: 0.7997\n",
      "Epoch 3/100\n",
      "12707/12707 [==============================] - 9s 672us/sample - loss: 0.3459 - acc: 0.8431 - val_loss: 0.4208 - val_acc: 0.8018\n",
      "Epoch 4/100\n",
      "12707/12707 [==============================] - 9s 677us/sample - loss: 0.3161 - acc: 0.8493 - val_loss: 0.4065 - val_acc: 0.8004\n",
      "Epoch 5/100\n",
      "12707/12707 [==============================] - 9s 679us/sample - loss: 0.2971 - acc: 0.8524 - val_loss: 0.4104 - val_acc: 0.7955\n",
      "Epoch 6/100\n",
      "12707/12707 [==============================] - 9s 688us/sample - loss: 0.2834 - acc: 0.8577 - val_loss: 0.4134 - val_acc: 0.7941\n",
      "Epoch 7/100\n",
      "12707/12707 [==============================] - 9s 671us/sample - loss: 0.2709 - acc: 0.8604 - val_loss: 0.4079 - val_acc: 0.7997\n",
      "Epoch 8/100\n",
      "12707/12707 [==============================] - 9s 669us/sample - loss: 0.2619 - acc: 0.8603 - val_loss: 0.4181 - val_acc: 0.7877\n",
      "Epoch 9/100\n",
      "12707/12707 [==============================] - 8s 665us/sample - loss: 0.2556 - acc: 0.8597 - val_loss: 0.4085 - val_acc: 0.7969\n",
      "Epoch 10/100\n",
      "12707/12707 [==============================] - 8s 667us/sample - loss: 0.2481 - acc: 0.8645 - val_loss: 0.4077 - val_acc: 0.7990\n",
      "Epoch 11/100\n",
      "12707/12707 [==============================] - 8s 669us/sample - loss: 0.2399 - acc: 0.8631 - val_loss: 0.4278 - val_acc: 0.7933\n",
      "Epoch 12/100\n",
      "12707/12707 [==============================] - 9s 675us/sample - loss: 0.2344 - acc: 0.8662 - val_loss: 0.4289 - val_acc: 0.7948\n",
      "Epoch 13/100\n",
      "12707/12707 [==============================] - 9s 676us/sample - loss: 0.2306 - acc: 0.8674 - val_loss: 0.4452 - val_acc: 0.7941\n",
      "Epoch 14/100\n",
      "12707/12707 [==============================] - 9s 671us/sample - loss: 0.2311 - acc: 0.8639 - val_loss: 0.4472 - val_acc: 0.8011\n",
      "idx, auc_micro, auc_macro:  0 0.9026267061143392 0.867624021875678\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12708 samples, validate on 1412 samples\n",
      "Epoch 1/100\n",
      "12708/12708 [==============================] - 9s 707us/sample - loss: 0.6601 - acc: 0.7169 - val_loss: 0.4869 - val_acc: 0.7897\n",
      "Epoch 2/100\n",
      "12708/12708 [==============================] - 8s 668us/sample - loss: 0.4016 - acc: 0.8233 - val_loss: 0.4191 - val_acc: 0.8109\n",
      "Epoch 3/100\n",
      "12708/12708 [==============================] - 8s 668us/sample - loss: 0.3513 - acc: 0.8413 - val_loss: 0.4103 - val_acc: 0.7953\n",
      "Epoch 4/100\n",
      "12708/12708 [==============================] - 8s 668us/sample - loss: 0.3147 - acc: 0.8521 - val_loss: 0.4041 - val_acc: 0.7989\n",
      "Epoch 5/100\n",
      "12708/12708 [==============================] - 8s 666us/sample - loss: 0.2969 - acc: 0.8536 - val_loss: 0.3925 - val_acc: 0.7953\n",
      "Epoch 6/100\n",
      "12708/12708 [==============================] - 8s 669us/sample - loss: 0.2844 - acc: 0.8584 - val_loss: 0.3881 - val_acc: 0.7989\n",
      "Epoch 7/100\n",
      "12708/12708 [==============================] - 9s 680us/sample - loss: 0.2692 - acc: 0.8631 - val_loss: 0.3747 - val_acc: 0.7897\n",
      "Epoch 8/100\n",
      "12708/12708 [==============================] - 8s 668us/sample - loss: 0.2649 - acc: 0.8582 - val_loss: 0.3773 - val_acc: 0.7989\n",
      "Epoch 9/100\n",
      "12708/12708 [==============================] - 8s 662us/sample - loss: 0.2571 - acc: 0.8586 - val_loss: 0.3757 - val_acc: 0.7939\n",
      "Epoch 10/100\n",
      "12708/12708 [==============================] - 8s 660us/sample - loss: 0.2460 - acc: 0.8622 - val_loss: 0.3863 - val_acc: 0.7925\n",
      "Epoch 11/100\n",
      "12708/12708 [==============================] - 9s 670us/sample - loss: 0.2426 - acc: 0.8654 - val_loss: 0.3800 - val_acc: 0.7897\n",
      "Epoch 12/100\n",
      "12708/12708 [==============================] - 9s 683us/sample - loss: 0.2418 - acc: 0.8628 - val_loss: 0.3802 - val_acc: 0.7890\n",
      "Epoch 13/100\n",
      "12708/12708 [==============================] - 9s 675us/sample - loss: 0.2345 - acc: 0.8637 - val_loss: 0.3893 - val_acc: 0.7932\n",
      "Epoch 14/100\n",
      "12708/12708 [==============================] - 9s 682us/sample - loss: 0.2302 - acc: 0.8648 - val_loss: 0.4102 - val_acc: 0.7996\n",
      "Epoch 15/100\n",
      "12708/12708 [==============================] - 9s 680us/sample - loss: 0.2309 - acc: 0.8670 - val_loss: 0.4005 - val_acc: 0.7861\n",
      "Epoch 16/100\n",
      "12708/12708 [==============================] - 9s 676us/sample - loss: 0.2267 - acc: 0.8693 - val_loss: 0.4204 - val_acc: 0.7890\n",
      "Epoch 17/100\n",
      "12708/12708 [==============================] - 9s 669us/sample - loss: 0.2270 - acc: 0.8676 - val_loss: 0.4212 - val_acc: 0.7854\n",
      "idx, auc_micro, auc_macro:  1 0.901848732836312 0.871340610449555\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12708 samples, validate on 1412 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12708/12708 [==============================] - 9s 678us/sample - loss: 0.6721 - acc: 0.7141 - val_loss: 0.4443 - val_acc: 0.8003\n",
      "Epoch 2/100\n",
      "12708/12708 [==============================] - 8s 637us/sample - loss: 0.4094 - acc: 0.8178 - val_loss: 0.4049 - val_acc: 0.8187\n",
      "Epoch 3/100\n",
      "12708/12708 [==============================] - 8s 642us/sample - loss: 0.3574 - acc: 0.8362 - val_loss: 0.3813 - val_acc: 0.8201\n",
      "Epoch 4/100\n",
      "12708/12708 [==============================] - 8s 640us/sample - loss: 0.3206 - acc: 0.8469 - val_loss: 0.4031 - val_acc: 0.8095\n",
      "Epoch 5/100\n",
      "12708/12708 [==============================] - 8s 656us/sample - loss: 0.2967 - acc: 0.8533 - val_loss: 0.3765 - val_acc: 0.8109\n",
      "Epoch 6/100\n",
      "12708/12708 [==============================] - 8s 640us/sample - loss: 0.2845 - acc: 0.8556 - val_loss: 0.3727 - val_acc: 0.8137\n",
      "Epoch 7/100\n",
      "12708/12708 [==============================] - 8s 645us/sample - loss: 0.2759 - acc: 0.8572 - val_loss: 0.3722 - val_acc: 0.8173\n",
      "Epoch 8/100\n",
      "12708/12708 [==============================] - 8s 639us/sample - loss: 0.2637 - acc: 0.8568 - val_loss: 0.3826 - val_acc: 0.8144\n",
      "Epoch 9/100\n",
      "12708/12708 [==============================] - 8s 642us/sample - loss: 0.2546 - acc: 0.8589 - val_loss: 0.3817 - val_acc: 0.8109\n",
      "Epoch 10/100\n",
      "12708/12708 [==============================] - 8s 639us/sample - loss: 0.2505 - acc: 0.8608 - val_loss: 0.3837 - val_acc: 0.8045\n",
      "Epoch 11/100\n",
      "12708/12708 [==============================] - 8s 643us/sample - loss: 0.2407 - acc: 0.8639 - val_loss: 0.3805 - val_acc: 0.8074\n",
      "Epoch 12/100\n",
      "12708/12708 [==============================] - 8s 646us/sample - loss: 0.2394 - acc: 0.8597 - val_loss: 0.3895 - val_acc: 0.8081\n",
      "Epoch 13/100\n",
      "12708/12708 [==============================] - 8s 653us/sample - loss: 0.2358 - acc: 0.8637 - val_loss: 0.3870 - val_acc: 0.8038\n",
      "Epoch 14/100\n",
      "12708/12708 [==============================] - 8s 638us/sample - loss: 0.2307 - acc: 0.8654 - val_loss: 0.4039 - val_acc: 0.8095\n",
      "Epoch 15/100\n",
      "12708/12708 [==============================] - 8s 644us/sample - loss: 0.2329 - acc: 0.8622 - val_loss: 0.3938 - val_acc: 0.8024\n",
      "Epoch 16/100\n",
      "12708/12708 [==============================] - 8s 642us/sample - loss: 0.2285 - acc: 0.8629 - val_loss: 0.3977 - val_acc: 0.8095\n",
      "Epoch 17/100\n",
      "12708/12708 [==============================] - 8s 639us/sample - loss: 0.2290 - acc: 0.8651 - val_loss: 0.4123 - val_acc: 0.8024\n",
      "idx, auc_micro, auc_macro:  2 0.9136937841568425 0.8871541981032027\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12708 samples, validate on 1412 samples\n",
      "Epoch 1/100\n",
      "12708/12708 [==============================] - 9s 698us/sample - loss: 0.6603 - acc: 0.7125 - val_loss: 0.4599 - val_acc: 0.7868\n",
      "Epoch 2/100\n",
      "12708/12708 [==============================] - 8s 648us/sample - loss: 0.4038 - acc: 0.8230 - val_loss: 0.4095 - val_acc: 0.8074\n",
      "Epoch 3/100\n",
      "12708/12708 [==============================] - 8s 658us/sample - loss: 0.3403 - acc: 0.8472 - val_loss: 0.4042 - val_acc: 0.8024\n",
      "Epoch 4/100\n",
      "12708/12708 [==============================] - 8s 657us/sample - loss: 0.3175 - acc: 0.8473 - val_loss: 0.3968 - val_acc: 0.8187\n",
      "Epoch 5/100\n",
      "12708/12708 [==============================] - 8s 656us/sample - loss: 0.2964 - acc: 0.8521 - val_loss: 0.3942 - val_acc: 0.8059\n",
      "Epoch 6/100\n",
      "12708/12708 [==============================] - 8s 655us/sample - loss: 0.2823 - acc: 0.8562 - val_loss: 0.3797 - val_acc: 0.8081\n",
      "Epoch 7/100\n",
      "12708/12708 [==============================] - 8s 646us/sample - loss: 0.2684 - acc: 0.8612 - val_loss: 0.3877 - val_acc: 0.8102\n",
      "Epoch 8/100\n",
      "12708/12708 [==============================] - 8s 653us/sample - loss: 0.2595 - acc: 0.8580 - val_loss: 0.3979 - val_acc: 0.8010\n",
      "Epoch 9/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.2529 - acc: 0.8639 - val_loss: 0.3886 - val_acc: 0.8059\n",
      "Epoch 10/100\n",
      "12708/12708 [==============================] - 8s 647us/sample - loss: 0.2504 - acc: 0.8590 - val_loss: 0.3940 - val_acc: 0.7989\n",
      "Epoch 11/100\n",
      "12708/12708 [==============================] - 8s 657us/sample - loss: 0.2431 - acc: 0.8618 - val_loss: 0.3841 - val_acc: 0.8159\n",
      "Epoch 12/100\n",
      "12708/12708 [==============================] - 8s 656us/sample - loss: 0.2375 - acc: 0.8624 - val_loss: 0.4014 - val_acc: 0.8067\n",
      "Epoch 13/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.2352 - acc: 0.8639 - val_loss: 0.4082 - val_acc: 0.8010\n",
      "Epoch 14/100\n",
      "12708/12708 [==============================] - 8s 646us/sample - loss: 0.2292 - acc: 0.8657 - val_loss: 0.4214 - val_acc: 0.7982\n",
      "Epoch 15/100\n",
      "12708/12708 [==============================] - 8s 650us/sample - loss: 0.2290 - acc: 0.8650 - val_loss: 0.4186 - val_acc: 0.8017\n",
      "Epoch 16/100\n",
      "12708/12708 [==============================] - 8s 643us/sample - loss: 0.2258 - acc: 0.8659 - val_loss: 0.4301 - val_acc: 0.8017\n",
      "idx, auc_micro, auc_macro:  3 0.9107006717010809 0.8834045389813048\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12708 samples, validate on 1412 samples\n",
      "Epoch 1/100\n",
      "12708/12708 [==============================] - 9s 699us/sample - loss: 0.6785 - acc: 0.7129 - val_loss: 0.5201 - val_acc: 0.7755\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12708/12708 [==============================] - 8s 644us/sample - loss: 0.4104 - acc: 0.8207 - val_loss: 0.4711 - val_acc: 0.7790\n",
      "Epoch 3/100\n",
      "12708/12708 [==============================] - 8s 666us/sample - loss: 0.3480 - acc: 0.8456 - val_loss: 0.4383 - val_acc: 0.7833\n",
      "Epoch 4/100\n",
      "12708/12708 [==============================] - 8s 650us/sample - loss: 0.3155 - acc: 0.8494 - val_loss: 0.4367 - val_acc: 0.7875\n",
      "Epoch 5/100\n",
      "12708/12708 [==============================] - 8s 644us/sample - loss: 0.2966 - acc: 0.8557 - val_loss: 0.4115 - val_acc: 0.7918\n",
      "Epoch 6/100\n",
      "12708/12708 [==============================] - 8s 652us/sample - loss: 0.2805 - acc: 0.8613 - val_loss: 0.4158 - val_acc: 0.7911\n",
      "Epoch 7/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.2703 - acc: 0.8590 - val_loss: 0.4116 - val_acc: 0.7904\n",
      "Epoch 8/100\n",
      "12708/12708 [==============================] - 8s 653us/sample - loss: 0.2635 - acc: 0.8609 - val_loss: 0.3995 - val_acc: 0.7925\n",
      "Epoch 9/100\n",
      "12708/12708 [==============================] - 8s 654us/sample - loss: 0.2518 - acc: 0.8650 - val_loss: 0.4070 - val_acc: 0.7847\n",
      "Epoch 10/100\n",
      "12708/12708 [==============================] - 8s 643us/sample - loss: 0.2482 - acc: 0.8629 - val_loss: 0.4010 - val_acc: 0.7783\n",
      "Epoch 11/100\n",
      "12708/12708 [==============================] - 9s 671us/sample - loss: 0.2439 - acc: 0.8685 - val_loss: 0.4107 - val_acc: 0.7854\n",
      "Epoch 12/100\n",
      "12708/12708 [==============================] - 8s 649us/sample - loss: 0.2357 - acc: 0.8703 - val_loss: 0.4165 - val_acc: 0.7868\n",
      "Epoch 13/100\n",
      "12708/12708 [==============================] - 8s 650us/sample - loss: 0.2346 - acc: 0.8694 - val_loss: 0.4084 - val_acc: 0.7904\n",
      "Epoch 14/100\n",
      "12708/12708 [==============================] - 8s 644us/sample - loss: 0.2321 - acc: 0.8676 - val_loss: 0.4104 - val_acc: 0.7918\n",
      "Epoch 15/100\n",
      "12708/12708 [==============================] - 8s 649us/sample - loss: 0.2302 - acc: 0.8655 - val_loss: 0.4263 - val_acc: 0.7932\n",
      "Epoch 16/100\n",
      "12708/12708 [==============================] - 8s 648us/sample - loss: 0.2273 - acc: 0.8710 - val_loss: 0.4339 - val_acc: 0.7890\n",
      "Epoch 17/100\n",
      "12708/12708 [==============================] - 8s 650us/sample - loss: 0.2254 - acc: 0.8675 - val_loss: 0.4223 - val_acc: 0.7861\n",
      "Epoch 18/100\n",
      "12708/12708 [==============================] - 8s 653us/sample - loss: 0.2232 - acc: 0.8687 - val_loss: 0.4367 - val_acc: 0.7925\n",
      "idx, auc_micro, auc_macro:  4 0.899763460103203 0.865574089224212\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12708 samples, validate on 1412 samples\n",
      "Epoch 1/100\n",
      "12708/12708 [==============================] - 9s 706us/sample - loss: 0.6735 - acc: 0.7114 - val_loss: 0.4778 - val_acc: 0.7861\n",
      "Epoch 2/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.4099 - acc: 0.8215 - val_loss: 0.4335 - val_acc: 0.8010\n",
      "Epoch 3/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.3502 - acc: 0.8410 - val_loss: 0.4333 - val_acc: 0.7911\n",
      "Epoch 4/100\n",
      "12708/12708 [==============================] - 8s 654us/sample - loss: 0.3189 - acc: 0.8495 - val_loss: 0.4169 - val_acc: 0.7996\n",
      "Epoch 5/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.3000 - acc: 0.8554 - val_loss: 0.4133 - val_acc: 0.7925\n",
      "Epoch 6/100\n",
      "12708/12708 [==============================] - 8s 653us/sample - loss: 0.2856 - acc: 0.8562 - val_loss: 0.3955 - val_acc: 0.7847\n",
      "Epoch 7/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.2732 - acc: 0.8580 - val_loss: 0.4072 - val_acc: 0.7882\n",
      "Epoch 8/100\n",
      "12708/12708 [==============================] - 8s 665us/sample - loss: 0.2654 - acc: 0.8604 - val_loss: 0.3903 - val_acc: 0.7911\n",
      "Epoch 9/100\n",
      "12708/12708 [==============================] - 8s 657us/sample - loss: 0.2553 - acc: 0.8622 - val_loss: 0.4011 - val_acc: 0.7847\n",
      "Epoch 10/100\n",
      "12708/12708 [==============================] - 8s 650us/sample - loss: 0.2487 - acc: 0.8625 - val_loss: 0.3982 - val_acc: 0.7925\n",
      "Epoch 11/100\n",
      "12708/12708 [==============================] - 8s 652us/sample - loss: 0.2464 - acc: 0.8619 - val_loss: 0.4047 - val_acc: 0.7996\n",
      "Epoch 12/100\n",
      "12708/12708 [==============================] - 8s 650us/sample - loss: 0.2416 - acc: 0.8616 - val_loss: 0.3833 - val_acc: 0.7918\n",
      "Epoch 13/100\n",
      "12708/12708 [==============================] - 8s 652us/sample - loss: 0.2365 - acc: 0.8621 - val_loss: 0.3939 - val_acc: 0.7960\n",
      "Epoch 14/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.2297 - acc: 0.8695 - val_loss: 0.4131 - val_acc: 0.7939\n",
      "Epoch 15/100\n",
      "12708/12708 [==============================] - 8s 649us/sample - loss: 0.2284 - acc: 0.8684 - val_loss: 0.4219 - val_acc: 0.7953\n",
      "Epoch 16/100\n",
      "12708/12708 [==============================] - 8s 663us/sample - loss: 0.2280 - acc: 0.8676 - val_loss: 0.4255 - val_acc: 0.7939\n",
      "Epoch 17/100\n",
      "12708/12708 [==============================] - 8s 657us/sample - loss: 0.2276 - acc: 0.8661 - val_loss: 0.4362 - val_acc: 0.7925\n",
      "Epoch 18/100\n",
      "12708/12708 [==============================] - 8s 656us/sample - loss: 0.2261 - acc: 0.8701 - val_loss: 0.4295 - val_acc: 0.7918\n",
      "Epoch 19/100\n",
      "12708/12708 [==============================] - 8s 648us/sample - loss: 0.2257 - acc: 0.8693 - val_loss: 0.4401 - val_acc: 0.7833\n",
      "Epoch 20/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.2223 - acc: 0.8685 - val_loss: 0.4342 - val_acc: 0.7868\n",
      "Epoch 21/100\n",
      "12708/12708 [==============================] - 8s 649us/sample - loss: 0.2205 - acc: 0.8709 - val_loss: 0.4447 - val_acc: 0.7946\n",
      "Epoch 22/100\n",
      "12708/12708 [==============================] - 8s 652us/sample - loss: 0.2212 - acc: 0.8691 - val_loss: 0.4627 - val_acc: 0.7918\n",
      "idx, auc_micro, auc_macro:  5 0.9019864135014324 0.8683200517124705\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12708 samples, validate on 1412 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12708/12708 [==============================] - 9s 702us/sample - loss: 0.6487 - acc: 0.7194 - val_loss: 0.4694 - val_acc: 0.7805\n",
      "Epoch 2/100\n",
      "12708/12708 [==============================] - 8s 654us/sample - loss: 0.4039 - acc: 0.8234 - val_loss: 0.4486 - val_acc: 0.7960\n",
      "Epoch 3/100\n",
      "12708/12708 [==============================] - 8s 651us/sample - loss: 0.3445 - acc: 0.8417 - val_loss: 0.4158 - val_acc: 0.8031\n",
      "Epoch 4/100\n",
      "12708/12708 [==============================] - 8s 646us/sample - loss: 0.3172 - acc: 0.8493 - val_loss: 0.4159 - val_acc: 0.8067\n",
      "Epoch 5/100\n",
      "12708/12708 [==============================] - 8s 649us/sample - loss: 0.2967 - acc: 0.8528 - val_loss: 0.4150 - val_acc: 0.8017\n",
      "Epoch 6/100\n",
      "12708/12708 [==============================] - 8s 654us/sample - loss: 0.2780 - acc: 0.8604 - val_loss: 0.4289 - val_acc: 0.7939\n",
      "Epoch 7/100\n",
      "12708/12708 [==============================] - 8s 653us/sample - loss: 0.2702 - acc: 0.8610 - val_loss: 0.4118 - val_acc: 0.8045\n",
      "Epoch 8/100\n",
      "12708/12708 [==============================] - 8s 644us/sample - loss: 0.2644 - acc: 0.8568 - val_loss: 0.4149 - val_acc: 0.7946\n",
      "Epoch 9/100\n",
      "12708/12708 [==============================] - 8s 659us/sample - loss: 0.2505 - acc: 0.8643 - val_loss: 0.4168 - val_acc: 0.8024\n",
      "Epoch 10/100\n",
      "12708/12708 [==============================] - 8s 658us/sample - loss: 0.2457 - acc: 0.8662 - val_loss: 0.4129 - val_acc: 0.8038\n",
      "Epoch 11/100\n",
      "12708/12708 [==============================] - 8s 650us/sample - loss: 0.2397 - acc: 0.8632 - val_loss: 0.4179 - val_acc: 0.7939\n",
      "Epoch 12/100\n",
      "12708/12708 [==============================] - 8s 647us/sample - loss: 0.2338 - acc: 0.8658 - val_loss: 0.4470 - val_acc: 0.7982\n",
      "Epoch 13/100\n",
      "12708/12708 [==============================] - 8s 647us/sample - loss: 0.2347 - acc: 0.8632 - val_loss: 0.4435 - val_acc: 0.8038\n",
      "Epoch 14/100\n",
      "12708/12708 [==============================] - 8s 649us/sample - loss: 0.2337 - acc: 0.8645 - val_loss: 0.4259 - val_acc: 0.8024\n",
      "Epoch 15/100\n",
      "12708/12708 [==============================] - 8s 650us/sample - loss: 0.2319 - acc: 0.8622 - val_loss: 0.4442 - val_acc: 0.8010\n",
      "Epoch 16/100\n",
      "12708/12708 [==============================] - 8s 650us/sample - loss: 0.2273 - acc: 0.8665 - val_loss: 0.4498 - val_acc: 0.7953\n",
      "Epoch 17/100\n",
      "12708/12708 [==============================] - 8s 660us/sample - loss: 0.2262 - acc: 0.8658 - val_loss: 0.4787 - val_acc: 0.7904\n",
      "idx, auc_micro, auc_macro:  6 0.9000192602460497 0.8670701653385047\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12708 samples, validate on 1412 samples\n",
      "Epoch 1/100\n",
      "12708/12708 [==============================] - 9s 715us/sample - loss: 0.6817 - acc: 0.7134 - val_loss: 0.4616 - val_acc: 0.7890\n",
      "Epoch 2/100\n",
      "12708/12708 [==============================] - 8s 661us/sample - loss: 0.4073 - acc: 0.8186 - val_loss: 0.4015 - val_acc: 0.8222\n",
      "Epoch 3/100\n",
      "12708/12708 [==============================] - 8s 659us/sample - loss: 0.3574 - acc: 0.8378 - val_loss: 0.3828 - val_acc: 0.8123\n",
      "Epoch 4/100\n",
      "12708/12708 [==============================] - 8s 654us/sample - loss: 0.3195 - acc: 0.8449 - val_loss: 0.3737 - val_acc: 0.8229\n",
      "Epoch 5/100\n",
      "12708/12708 [==============================] - 8s 667us/sample - loss: 0.3019 - acc: 0.8497 - val_loss: 0.3668 - val_acc: 0.8144\n",
      "Epoch 6/100\n",
      "12708/12708 [==============================] - 8s 666us/sample - loss: 0.2805 - acc: 0.8554 - val_loss: 0.3600 - val_acc: 0.8194\n",
      "Epoch 7/100\n",
      "12708/12708 [==============================] - 8s 664us/sample - loss: 0.2751 - acc: 0.8567 - val_loss: 0.3605 - val_acc: 0.8109\n",
      "Epoch 8/100\n",
      "12708/12708 [==============================] - 9s 671us/sample - loss: 0.2664 - acc: 0.8580 - val_loss: 0.3605 - val_acc: 0.8187\n",
      "Epoch 9/100\n",
      "12708/12708 [==============================] - 9s 670us/sample - loss: 0.2535 - acc: 0.8644 - val_loss: 0.3594 - val_acc: 0.8088\n",
      "Epoch 10/100\n",
      "12708/12708 [==============================] - 9s 681us/sample - loss: 0.2492 - acc: 0.8601 - val_loss: 0.3555 - val_acc: 0.8123\n",
      "Epoch 11/100\n",
      "12708/12708 [==============================] - 8s 665us/sample - loss: 0.2427 - acc: 0.8604 - val_loss: 0.3658 - val_acc: 0.8208\n",
      "Epoch 12/100\n",
      "12708/12708 [==============================] - 9s 678us/sample - loss: 0.2397 - acc: 0.8605 - val_loss: 0.3737 - val_acc: 0.8095\n",
      "Epoch 13/100\n",
      "12708/12708 [==============================] - 9s 695us/sample - loss: 0.2352 - acc: 0.8636 - val_loss: 0.3825 - val_acc: 0.8194\n",
      "Epoch 14/100\n",
      "12708/12708 [==============================] - 9s 691us/sample - loss: 0.2352 - acc: 0.8628 - val_loss: 0.3769 - val_acc: 0.8194\n",
      "Epoch 15/100\n",
      "12708/12708 [==============================] - 9s 706us/sample - loss: 0.2320 - acc: 0.8627 - val_loss: 0.3822 - val_acc: 0.8159\n",
      "Epoch 16/100\n",
      "12708/12708 [==============================] - 9s 690us/sample - loss: 0.2311 - acc: 0.8653 - val_loss: 0.3854 - val_acc: 0.8074\n",
      "Epoch 17/100\n",
      "12708/12708 [==============================] - 9s 690us/sample - loss: 0.2267 - acc: 0.8658 - val_loss: 0.3997 - val_acc: 0.8109\n",
      "Epoch 18/100\n",
      "12708/12708 [==============================] - 9s 690us/sample - loss: 0.2295 - acc: 0.8581 - val_loss: 0.4113 - val_acc: 0.8144\n",
      "Epoch 19/100\n",
      "12708/12708 [==============================] - 9s 683us/sample - loss: 0.2252 - acc: 0.8669 - val_loss: 0.3848 - val_acc: 0.8116\n",
      "Epoch 20/100\n",
      "12708/12708 [==============================] - 8s 661us/sample - loss: 0.2236 - acc: 0.8652 - val_loss: 0.4018 - val_acc: 0.8208\n",
      "idx, auc_micro, auc_macro:  7 0.9191912301679652 0.8945739781232009\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12708 samples, validate on 1412 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12708/12708 [==============================] - 9s 707us/sample - loss: 0.6755 - acc: 0.7101 - val_loss: 0.4453 - val_acc: 0.7989\n",
      "Epoch 2/100\n",
      "12708/12708 [==============================] - 9s 669us/sample - loss: 0.4134 - acc: 0.8191 - val_loss: 0.4047 - val_acc: 0.8067\n",
      "Epoch 3/100\n",
      "12708/12708 [==============================] - 8s 657us/sample - loss: 0.3438 - acc: 0.8437 - val_loss: 0.3817 - val_acc: 0.8144\n",
      "Epoch 4/100\n",
      "12708/12708 [==============================] - 8s 663us/sample - loss: 0.3279 - acc: 0.8458 - val_loss: 0.3783 - val_acc: 0.8229\n",
      "Epoch 5/100\n",
      "12708/12708 [==============================] - 8s 654us/sample - loss: 0.3035 - acc: 0.8520 - val_loss: 0.3705 - val_acc: 0.8364\n",
      "Epoch 6/100\n",
      "12708/12708 [==============================] - 8s 663us/sample - loss: 0.2895 - acc: 0.8557 - val_loss: 0.3618 - val_acc: 0.8137\n",
      "Epoch 7/100\n",
      "12708/12708 [==============================] - 9s 689us/sample - loss: 0.2780 - acc: 0.8541 - val_loss: 0.3562 - val_acc: 0.8244\n",
      "Epoch 8/100\n",
      "12708/12708 [==============================] - 8s 667us/sample - loss: 0.2648 - acc: 0.8573 - val_loss: 0.3541 - val_acc: 0.8208\n",
      "Epoch 9/100\n",
      "12708/12708 [==============================] - 8s 655us/sample - loss: 0.2546 - acc: 0.8577 - val_loss: 0.3615 - val_acc: 0.8180\n",
      "Epoch 10/100\n",
      "12708/12708 [==============================] - 9s 678us/sample - loss: 0.2509 - acc: 0.8610 - val_loss: 0.3498 - val_acc: 0.8166\n",
      "Epoch 11/100\n",
      "12708/12708 [==============================] - 8s 657us/sample - loss: 0.2474 - acc: 0.8595 - val_loss: 0.3571 - val_acc: 0.8081\n",
      "Epoch 12/100\n",
      "12708/12708 [==============================] - 8s 657us/sample - loss: 0.2408 - acc: 0.8618 - val_loss: 0.3567 - val_acc: 0.8102\n",
      "Epoch 13/100\n",
      "12708/12708 [==============================] - 8s 657us/sample - loss: 0.2362 - acc: 0.8636 - val_loss: 0.3579 - val_acc: 0.8102\n",
      "Epoch 14/100\n",
      "12708/12708 [==============================] - 8s 658us/sample - loss: 0.2319 - acc: 0.8645 - val_loss: 0.3670 - val_acc: 0.8194\n",
      "Epoch 15/100\n",
      "12708/12708 [==============================] - 8s 660us/sample - loss: 0.2281 - acc: 0.8642 - val_loss: 0.3658 - val_acc: 0.8251\n",
      "Epoch 16/100\n",
      "12708/12708 [==============================] - 8s 656us/sample - loss: 0.2308 - acc: 0.8665 - val_loss: 0.3859 - val_acc: 0.8166\n",
      "Epoch 17/100\n",
      "12708/12708 [==============================] - 9s 679us/sample - loss: 0.2266 - acc: 0.8659 - val_loss: 0.3683 - val_acc: 0.8194\n",
      "Epoch 18/100\n",
      "12708/12708 [==============================] - 9s 695us/sample - loss: 0.2269 - acc: 0.8662 - val_loss: 0.3748 - val_acc: 0.8123\n",
      "Epoch 19/100\n",
      "12708/12708 [==============================] - 9s 670us/sample - loss: 0.2265 - acc: 0.8634 - val_loss: 0.3854 - val_acc: 0.8159\n",
      "Epoch 20/100\n",
      "12708/12708 [==============================] - 8s 659us/sample - loss: 0.2276 - acc: 0.8662 - val_loss: 0.3801 - val_acc: 0.8130\n",
      "idx, auc_micro, auc_macro:  8 0.9183784377532923 0.8936353008312377\n",
      "________DNN_________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Inputlayer (InputLayer)      (None, 3390)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 512)               1736192   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,871,106\n",
      "Trainable params: 1,869,570\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 12709 samples, validate on 1411 samples\n",
      "Epoch 1/100\n",
      "12709/12709 [==============================] - 9s 744us/sample - loss: 0.6800 - acc: 0.7138 - val_loss: 0.4558 - val_acc: 0.7888\n",
      "Epoch 2/100\n",
      "12709/12709 [==============================] - 9s 680us/sample - loss: 0.4186 - acc: 0.8160 - val_loss: 0.4017 - val_acc: 0.8129\n",
      "Epoch 3/100\n",
      "12709/12709 [==============================] - 9s 674us/sample - loss: 0.3527 - acc: 0.8378 - val_loss: 0.3975 - val_acc: 0.8221\n",
      "Epoch 4/100\n",
      "12709/12709 [==============================] - 9s 677us/sample - loss: 0.3226 - acc: 0.8501 - val_loss: 0.3884 - val_acc: 0.8143\n",
      "Epoch 5/100\n",
      "12709/12709 [==============================] - 9s 681us/sample - loss: 0.2979 - acc: 0.8544 - val_loss: 0.3793 - val_acc: 0.8164\n",
      "Epoch 6/100\n",
      "12709/12709 [==============================] - 9s 682us/sample - loss: 0.2879 - acc: 0.8541 - val_loss: 0.3781 - val_acc: 0.8086\n",
      "Epoch 7/100\n",
      "12709/12709 [==============================] - 9s 679us/sample - loss: 0.2749 - acc: 0.8559 - val_loss: 0.3662 - val_acc: 0.8136\n",
      "Epoch 8/100\n",
      "12709/12709 [==============================] - 9s 686us/sample - loss: 0.2668 - acc: 0.8587 - val_loss: 0.3851 - val_acc: 0.8143\n",
      "Epoch 9/100\n",
      "12709/12709 [==============================] - 9s 672us/sample - loss: 0.2609 - acc: 0.8613 - val_loss: 0.3807 - val_acc: 0.8072\n",
      "Epoch 10/100\n",
      "12709/12709 [==============================] - 9s 676us/sample - loss: 0.2532 - acc: 0.8633 - val_loss: 0.3753 - val_acc: 0.8179\n",
      "Epoch 11/100\n",
      "12709/12709 [==============================] - 9s 673us/sample - loss: 0.2465 - acc: 0.8605 - val_loss: 0.3950 - val_acc: 0.8094\n",
      "Epoch 12/100\n",
      "12709/12709 [==============================] - 9s 680us/sample - loss: 0.2408 - acc: 0.8628 - val_loss: 0.3859 - val_acc: 0.8065\n",
      "Epoch 13/100\n",
      "12709/12709 [==============================] - 9s 679us/sample - loss: 0.2384 - acc: 0.8636 - val_loss: 0.3934 - val_acc: 0.8115\n",
      "Epoch 14/100\n",
      "12709/12709 [==============================] - 9s 681us/sample - loss: 0.2344 - acc: 0.8635 - val_loss: 0.4036 - val_acc: 0.8065\n",
      "Epoch 15/100\n",
      "12709/12709 [==============================] - 9s 673us/sample - loss: 0.2332 - acc: 0.8635 - val_loss: 0.4088 - val_acc: 0.8079\n",
      "Epoch 16/100\n",
      "12709/12709 [==============================] - 9s 670us/sample - loss: 0.2309 - acc: 0.8643 - val_loss: 0.4149 - val_acc: 0.8065\n",
      "Epoch 17/100\n",
      "12709/12709 [==============================] - 9s 670us/sample - loss: 0.2252 - acc: 0.8679 - val_loss: 0.4252 - val_acc: 0.8094\n",
      "idx, auc_micro, auc_macro:  9 0.9130583282812328 0.8856984633300423\n",
      "auc_micro_all, auc_macro_all:  0.9082562681066376 0.8786994314926164\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
